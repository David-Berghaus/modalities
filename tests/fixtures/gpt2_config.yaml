llm_model_conf:
  sample_key: input_ids
  prediction_key: "logits"
  block_size: 1024
  vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
  n_layer: 12
  n_head: 12
  ffn_hidden: 2048
  n_embd: 768
  dropout: 0.0
  bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  attention:
    attention_type: pytorch_flash_attention
    scaling_factor: 3
  activation: fused_swiglu
  epsilon: 1e-5
  weight_init:
    mean: 0.0
    std: 0.02

running_env_conf:
  process_group_backend: "nccl"
  local_rank: ${oc.env:LOCAL_RANK}
