wandb:
  project_name: llm_gym

data:
  sample_key: "input_ids"
  target_key: "target_ids"
  sequence_len: 128
  train_dataloader:
    type_hint: LLMDataLoader
    config:
      num_workers: 2
      pin_memory: true
      shuffle: false
      dataloader_tag: "train"
      dataset:
        type_hint: MemMapDataset
        config:
          raw_data_path: ./data/lorem_ipsum.jsonl
          index_path: ./data/lorem_ipsum.idx
          block_size: ${data.sequence_len}
          jq_pattern: ".text"
          sample_key: ${data.sample_key}
          tokenizer:
            type_hint: GPT2TokenizerFast
            config:
              tokenizer_file: ./data/tokenizer/tokenizer.json
      batch_sampler:
        type_hint: BatchSampler
        config:
          batch_size: 32 # per rank
          drop_last: false
          sampler:
            type_hint: DistributedSampler
            config:
              rank: ${training.global_rank}
              num_replicas: ${training.world_size}
              shuffle: true
      collate_fn:
        type_hint: GPT2LLMCollator
        config:
          sample_key: ${data.sample_key}
          target_key: ${data.target_key}
  eval_dataloaders:
    - type_hint: LLMDataLoader
      config:
        batch_size: 21
        num_workers: 2
        pin_memory: true
        shuffle: false
        dataloader_tag: "val"
        dataset: ${data.train_dataloader.config.dataset}
        batch_sampler:
          type_hint: BatchSampler
          config:
            batch_size: 32 # per rank
            drop_last: false
            sampler:
              type_hint: DistributedSampler
              config:
                rank: ${training.global_rank}
                num_replicas: ${training.world_size}
                shuffle: true
        collate_fn: ${data.train_dataloader.config.collate_fn}
    - type_hint: LLMDataLoader
      config:
        batch_size: 21
        num_workers: 2
        pin_memory: true
        shuffle: false
        dataloader_tag: "test"
        dataset: ${data.train_dataloader.config.dataset}
        batch_sampler:
          type_hint: BatchSampler
          config:
            batch_size: 32 # per rank
            drop_last: false
            sampler:
              type_hint: DistributedSampler
              config:
                rank: ${training.global_rank}
                num_replicas: ${training.world_size}
                shuffle: true
        collate_fn: ${data.train_dataloader.config.collate_fn}

training:
  process_group_backend: "nccl"
  num_training_samples: 5
  callback_interval_in_samples: 1
  local_rank: ${oc.env:LOCAL_RANK}
  global_rank: ${oc.env:RANK}
  world_size: ${oc.env:WORLD_SIZE}
  main_rank: 0
  train_batch_size: ${data.train_dataloader.config.batch_sampler.config.batch_size}


checkpoint:
  dir_path: data/checkpoints
  checkpointing_rank: ${training.main_rank}

loss:
  type_hint: CLMCrossEntropyLoss
  config:
    target_key: ${data.target_key}
    prediction_key: ${model.config.prediction_key}

running_env:
  type_hint: FSDPRunningEnv
  config:
    process_group_backend: ${training.process_group_backend}
    local_rank: ${oc.env:LOCAL_RANK}

model:
  type_hint: GPT2LLM
  config:
    sample_key: ${data.sample_key}
    prediction_key: "logits"
    block_size: ${data.sequence_len}
    vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: 2
    n_head: 4
    ffn_hidden: 128
    n_embd: 128
    dropout: 0.0
    bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
    attention:
      attention_type: pytorch_flash_attention
      scaling_factor: 3
    activation: fused_swiglu
    epsilon: 1e-5
    weight_init:
      mean: 0.0
      std: 0.02

scheduler:
  type_hint: StepLR
  config:
    step_size: 1
    gamma: 0.1

optimizer:
  type_hint: AdamW
  config:
    lr: 0.0001
