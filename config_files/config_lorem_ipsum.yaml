wandb:
  project_name: llm_gym

training:
  train_dataloader:
    type_hint: LLMDataLoader
    config:
      batch_size: 32
      num_workers: 2
      pin_memory: true
      shuffle: false
      dataset_tag: "train"
      dataset:
        type_hint: MemMapDataset
        config:
          raw_data_path: ./data/lorem_ipsum.jsonl
          tokenizer_path: ./data/tokenizer/tokenizer.json
          jq_pattern: ".text"
      sampler:
        type_hint: DistributedSampler
        config:
          rank: ${training.global_rank}
          num_replicas: ${training.world_size}
          shuffle: true
      collate_fn:
        type_hint: GPT2LLMCollator
        config:
          sample_key: "input_ids"
          target_key: "target_ids"
  evaluation_dataloaders:
    val:
      type_hint: LLMDataLoader
      config:
        batch_size: 21
        num_workers: 2
        pin_memory: true
        shuffle: false
        dataset_tag: "val"
        dataset:
          type_hint: MemMapDataset
          config:
            raw_data_path: ./data/lorem_ipsum.jsonl
            tokenizer_path: ./data/tokenizer/tokenizer.json
            jq_pattern: ".text"
        sampler:
          type_hint: DistributedSampler
          config:
            rank: ${training.global_rank}
            num_replicas: ${training.world_size}
            shuffle: false
        collate_fn: ${training.train_dataloader.config.collate_fn}
    test:
      type_hint: LLMDataLoader
      config:
        batch_size: 21
        num_workers: 2
        pin_memory: true
        shuffle: false
        dataset_tag: "test"
        dataset:
          type_hint: MemMapDataset
          config:
            raw_data_path: ./data/lorem_ipsum.jsonl
            tokenizer_path: ./data/tokenizer/tokenizer.json
            jq_pattern: ".text"
        sampler:
          type_hint: DistributedSampler
          config:
            rank: ${training.global_rank}
            num_replicas: ${training.world_size}
            shuffle: false
        collate_fn: ${training.train_dataloader.config.collate_fn}
  process_group_backend: "nccl"
  num_training_samples: 5
  callback_interval_in_samples: 1
  local_rank: ${oc.env:LOCAL_RANK}
  global_rank: ${oc.env:RANK}
  world_size: ${oc.env:WORLD_SIZE}
  main_rank: 0

checkpoint:
  dir_path: data/checkpoints
  checkpointing_rank: ${training.main_rank}

loss:
  type_hint: CLMCrossEntropyLoss
  config:
    target_key: ${training.train_dataloader.config.collate_fn.config.target_key}
    prediction_key: ${model.config.config.prediction_key}

runner:
  type_hint: FSDPRunner
  config:
    process_group_backend: ${training.process_group_backend}

model:
  type_hint: GPT2LLM
  config:
    config:
      sample_key: ${training.train_dataloader.config.collate_fn.config.sample_key}
      prediction_key: "logits"
      block_size: 1024
      vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
      n_layer: 12
      n_head: 12
      ffn_hidden: 2048
      n_embd: 768
      dropout: 0.0
      bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
      attention:
        attention_type: pytorch_flash_attention
        scaling_factor: 3
      activation: fused_swiglu
      epsilon: 1e-5
      weight_init:
        mean: 0.0
        std: 0.02

scheduler:
  type_hint: StepLR
  config:
    step_size: 1
    gamma: 0.1

optimizer:
  type_hint: AdamW
  config:
    lr: 0.0001
