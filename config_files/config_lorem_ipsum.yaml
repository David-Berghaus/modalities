

# modalities_setup:
#   run_mode: FROM_SCRATCH
#   settings:
#     global_num_seen_samples: 0

# wandb:
#   project_name: modalities
#   mode: OFFLINE

# data:
#   sample_key: "input_ids"
#   target_key: "target_ids"
#   sequence_len: 256
#   train_dataloader:
#     type_hint: LLMDataLoader
#     config:
#       num_workers: 2
#       pin_memory: true
#       shuffle: false
#       dataloader_tag: "train"
#       dataset:
#         type_hint: MemMapDataset
#         config:
#           raw_data_path: data/lorem_ipsum.jsonl
#           index_path: data/lorem_ipsum.idx
#           block_size: ${data.sequence_len}
#           jq_pattern: ".text"
#           sample_key: ${data.sample_key}
#           tokenizer:
#             type_hint: GPT2TokenizerFast
#             config:
#               tokenizer_file: data/tokenizer/tokenizer_gpt2.json
#       batch_sampler:
#         type_hint: BatchSampler
#         config:
#           batch_size: 3
#           drop_last: false
#           sampler:
#             type_hint: DistributedSampler
#             config:
#               rank: ${training.global_rank}
#               num_replicas: ${training.world_size}
#               shuffle: true
#       collate_fn:
#         type_hint: GPT2LLMCollator
#         config:
#           sample_key: ${data.sample_key}
#           target_key: ${data.target_key}
#   eval_dataloaders:
#     - type_hint: LLMDataLoader
#       config:
#         num_workers: 2
#         pin_memory: true
#         shuffle: false
#         dataloader_tag: "val"
#         dataset: ${data.train_dataloader.config.dataset}
#         batch_sampler:
#           type_hint: BatchSampler
#           config:
#             batch_size: 3
#             drop_last: false
#             sampler:
#               type_hint: DistributedSampler
#               config:
#                 rank: ${training.global_rank}
#                 num_replicas: ${training.world_size}
#                 shuffle: true
#         collate_fn: ${data.train_dataloader.config.collate_fn}
#     - type_hint: LLMDataLoader
#       config:
#         num_workers: 2
#         pin_memory: true
#         shuffle: false
#         dataloader_tag: "test"
#         dataset: ${data.train_dataloader.config.dataset}
#         batch_sampler:
#           type_hint: BatchSampler
#           config:
#             batch_size: 3
#             drop_last: false
#             sampler:
#               type_hint: DistributedSampler
#               config:
#                 rank: ${training.global_rank}
#                 num_replicas: ${training.world_size}
#                 shuffle: true
#         collate_fn: ${data.train_dataloader.config.collate_fn}

# training:
#   process_group_backend: "nccl"
#   global_num_training_samples: 12
#   callback_interval_in_samples: 6
#   local_rank: ${oc.env:LOCAL_RANK}
#   global_rank: ${oc.env:RANK}
#   world_size: ${oc.env:WORLD_SIZE}
#   main_rank: 0
#   local_train_micro_batch_size: ${data.train_dataloader.config.batch_sampler.config.batch_size}
#   global_num_seen_samples: ${modalities_setup.settings.global_num_seen_samples}
#   gradient_acc_step: 1
#   do_apply_activation_checkpointing: True

checkpointing:
  component_key: checkpointing
  variant_key: default
  config:
    checkpointing_strategy:
      component_key: checkpointing_strategy
      variant_key: save_k_most_recent_checkpoints_strategy
      config:
        k: -1   # -1 to save all checkpoints
    checkpointing_execution:
      component_key: checkpointing_execution
      variant_key: fsdp_to_disc_checkpointing
      config:
        checkpoint_path: data/checkpoints
        global_rank: ${modalities_env:RANK}
        experiment_id: "abc" # TODO set this via env var maybe?
        running_env:
          instance_key: running_env
          pass_type: BY_REFERENCE

# resolving class types via different enums sucks...
loss:
  component_key: loss
  variant_key: clm_cross_entropy_loss
  config:
    target_key: target_ids
    prediction_key: logits

running_env:
  component_key: running_env
  variant_key: fsdp_running_env
  config:
    process_group_backend: nccl
    local_rank: ${modalities_env:LOCAL_RANK}
    mixed_precision_settings: FP_16
    sharding_strategy: FULL_SHARD
    block_names: [GPT2Block]

model:
  component_key: model
  variant_key: gpt2
  config:
    sample_key: "input_ids" # TODO reference this
    prediction_key: "logits" # TODO reference this
    block_size: 256  # TODO reference this (same as sequence length)
    vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: 2
    n_head: 4
    ffn_hidden: 128
    n_embd: 128
    dropout: 0.0
    bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
    attention:
      attention_type: default_attention # pytorch_flash_attention
      scaling_factor: 3
    activation: gelu
    epsilon: 1e-5
    weight_init:
      mean: 0.0
      std: 0.02

# scheduler:
#   type_hint: StepLR
#   config:
#     step_size: 1
#     gamma: 0.1

optimizer:  
  component_key: optimizer
  variant_key: adam_w
  config:
    lr: 0.0001
    model: 
      instance_key: model
      pass_type: BY_REFERENCE


tokenizer:
  component_key: tokenizer
  variant_key: gpt2_tokenizer_fast
  config:
    tokenizer_file: data/tokenizer/tokenizer_gpt2.json

sampler:
  component_key: sampler
  variant_key: distributed_sampler
  config:
    rank: ${modalities_env:RANK}
    num_replicas: ${modalities_env:WORLD_SIZE}
    shuffle: true
    dataset:
      instance_key: dataset
      pass_type: BY_REFERENCE

  
dataset:  
  component_key: dataset
  variant_key: mem_map_dataset
  config:
    raw_data_path: data/lorem_ipsum.jsonl
    index_path: data/lorem_ipsum.idx
    block_size: 256 # TODO reference this (same as sequence length)
    jq_pattern: ".text"
    sample_key: "input_ids" # TODO reference this
    tokenizer:
      instance_key: tokenizer
      pass_type: BY_REFERENCE

batch_sampler:
  component_key: batch_sampler
  variant_key: default
  config:
    batch_size: 3
    drop_last: false
    sampler:
      instance_key: sampler
      pass_type: BY_REFERENCE
  
collate_fn:  
  component_key: collate_fn
  variant_key: gpt_2_llm_collator
  config:
    sample_key: input_ids # TODO reference this
    target_key: target_ids # TODO reference this



train_dataloader:
  component_key: data_loader
  variant_key: llm_data_loader
  config:
    num_workers: 2
    pin_memory: true
    shuffle: false
    dataloader_tag: "train"
    dataset:
      instance_key: dataset
      pass_type: BY_REFERENCE
    batch_sampler:
      instance_key: batch_sampler
      pass_type: BY_REFERENCE 
    collate_fn:
      instance_key: collate_fn
      pass_type: BY_REFERENCE 