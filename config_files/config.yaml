data:
  dataset_dir_path: /raid/s3/opengptx/mehdi/temp/temp_data
  sample_key: "input_ids"
  target_key: "target_ids"
  sequence_len: 1024
  dataloader:
    train_dataset_tag: "train"
    val_dataset_tag: "val"
    test_dataset_tag: "test"
    cuda_kwargs:
      num_workers: 2
      pin_memory: true
      shuffle: false

wandb:
  project_name: llm_gym

training:
  process_group_backend: "nccl"
  num_training_batches: 7000
  num_batches_per_training_sequence: 100
  eval_interval_in_batches: 2
  training_batch_size: 32
  evaluation_batch_size: 21
  test_batch_size: 21
  local_rank: ${oc.env:LOCAL_RANK}
  global_rank: ${oc.env:RANK}
  world_size: ${oc.env:WORLD_SIZE}
  main_rank: 0

checkpoint:
  dir_path: checkpoint
  checkpointing_rank: ${training.main_rank}

loss:
  type_hint: CLMCrossEntropyLoss
  config:
    target_key: ${data.target_key}
    prediction_key: ${model.config.config.prediction_key}

runner:
  type_hint: FSDPRunner
  config:
    process_group_backend: ${training.process_group_backend}

model:
  type_hint: GPT2LLM
  config:
    config:
      sample_key: ${data.sample_key}
      prediction_key: "logits"
      block_size: ${data.sequence_len}
      vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
      n_layer: 12
      n_head: 12
      ffn_hidden: 2048
      n_embd: 768
      dropout: 0.0
      bias: true # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
      attention:
        attention_type: pytorch_flash_attention
        scaling_factor: 3
      activation: fused_swiglu
      epsilon: 1e-5
      weight_init:
        mean: 0.0
        std: 0.02

scheduler:
  type_hint: StepLR
  config:
    step_size: 1
    gamma: 0.1

optimizer:
  type_hint: AdamW
  config:
    lr: 0.0001
